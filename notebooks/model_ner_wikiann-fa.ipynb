{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Project Overview\n",
    "\n",
    "This project focuses on building a Named Entity Recognition (NER) system for Persian (Farsi) text \n",
    "\n",
    "by fine-tuning a transformer-based model on the WikiAnn-fa dataset.\n",
    "\n",
    "The goal is to automatically identify and extract named entities such as persons (PER), locations \n",
    "\n",
    "(LOC), and organizations (ORG) from Persian text.\n",
    "\n",
    "The model is trained using Hugging Face Transformers and PyTorch, and the final system supports \n",
    "\n",
    "both offline inference and deployment as a RESTful API using FastAPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This cell installs the required Python libraries for training and evaluating the NER model:\n",
    "- `datasets` for loading and processing the WikiAnn-fa dataset\n",
    "- `transformers` for fine-tuning a pretrained Transformer model\n",
    "- `evaluate` and `seqeval` for computing NER sequence-labeling metrics (e.g., precision, recall, F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14647,
     "status": "ok",
     "timestamp": 1766060217409,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "-KR0O3u6XQXU",
    "outputId": "6f81caa0-8d04-4878-e998-bf8114144e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: seqeval in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: datasets in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (2.3.5)\n",
      "Requirement already satisfied: dill in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from seqeval) (1.8.0)\n",
      "Requirement already satisfied: filelock in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from datasets) (3.20.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.6.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: colorama in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from pandas->evaluate) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\resume_poroject\\project\\machin learning\\llm_project\\persian_ner\\ner-wikiann-fa\\p113\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install evaluate seqeval datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This cell imports the core libraries required for the project, including PyTorch for model training, Hugging Face Transformers for token classification (NER), Datasets for loading the WikiAnn-fa dataset, and evaluation utilities for computing NER metrics. It also includes supporting libraries for numerical operations and configuration handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1766060217714,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "e28e5836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\resume_poroject\\project\\Machin Learning\\LLM_Project\\persian_ner\\ner-wikiann-fa\\p113\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "This cell loads the Persian (Farsi) Named Entity Recognition dataset from WikiAnn using the Hugging Face Datasets library. The dataset provides token-level annotations for entities such as persons, locations, and organizations, and is used for training, validation, and evaluation of the NER model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "71fd6126b670406481893f3fcd6223e3",
      "80312a0ceea34148abc1bad13be5ad48",
      "7b0872ddd25e40edb64952de9e092a87",
      "a363910aa28048c4ae4067cfd2010bed",
      "3a5bc031a8a64ea79e2eeb3285a15b84",
      "12f0a93ebb624534a0ef869f6fdff0e2",
      "c2c53bdacf54479695f9785bfc0d6526",
      "09c277924455447f9921e36b440d2e87",
      "7f0d1aeaeeab46fdba496779ef2fa35c",
      "acc8ac1a45df4c5b8b59a87389773bce",
      "7138c373ccd6492d978f1caa4b4c8838",
      "b53c6e88776f4b268942eec826dad5f1",
      "aec19c2fc04e4b8882d61a9efece549f",
      "babab39af506490093a3e7d99d84f756",
      "e042ff21440e4bf480cd109d5760dbc9",
      "31ed030aaeb641bf9b15d12938b21789",
      "185482b3ffa74e5f8e5269abe16d50ed",
      "83c2fc80b80a44a199a7a9aacea8eb7d",
      "820328bb4717407889b47b0b327893fb",
      "ef780987b7eb47bd8beaf87812a686aa",
      "81831f38e0f44f4d96a8af0910160675",
      "09d8b4622f294c458f865871419a8c5c",
      "254d0ad925fe47c197e9770b08ffd4bd",
      "bcaf0184d127435ebbc077993349e3e0",
      "b434aff24bd84928a51ed7f8c7cbab04",
      "10e3b30fbbc24bbfba0174c8b74eb5cc",
      "cd51a41c658843ad999b88490e2b8882",
      "7610a0bc2dc14d6c967c359e0ebadcd3",
      "41db09c9dce44cc0ae299c53c8b2a4a2",
      "68173aae3ad1460897cc32d620fcf5e2",
      "d87e84cbf9214dbaad7019b94d00a24b",
      "0cc6bdb7a2d44557b41b4020c9df81c1",
      "4be041110ce944419cd352e13bad792c",
      "aff56a4724b649859c300e8ec5d0195f",
      "c5f413228a63468cb96126213e6c782c",
      "9d7bbfb654fa4aeab36ddd62c1a68aed",
      "bef785a48bd0492db9cd30ba165a5682",
      "e8c129d9751e4cc18f539b4c165e7c42",
      "714fbb46232046b291dd360df71b7190",
      "ffa2e23e64f2473ebb9b47bc95d20b18",
      "0f285adbd1264b659bac534f2f251d08",
      "469d6060b4204acbb49ec1ebb0b82409",
      "6b503e58895643f792b01c65c12d4f05",
      "225bdc3acd8845e8beea5dc6c044619d",
      "9834329952de43d89215ec49830fe498",
      "7e7fbb74d311452894d112ae18ebf6f5",
      "6790456c86124d4fbd1a14db12496ad8",
      "22fb5ce4ebad4855995960687dfb537e",
      "1a180b46cbd440b4a4c4ff793f40d7f2",
      "c817bc43a3e54497812cd5857ca30644",
      "4cfa9280f2664bc883ef1d0d44bc2a6f",
      "82a327de08384c1d8342d872e5b15185",
      "126566aae3ea4a218b740f30e2d344e8",
      "9d50d8fc984e45a5a79ecf5b425f8c24",
      "bc2200004e734e269514aa1363b6ae2e",
      "7e51e21547bf4890991d4948ccbdf717",
      "82909a47e496406494f36e5c0f122442",
      "d7613bc8f27c44a3a004beb619a826e3",
      "b76bd8f1a3bb40048195c0acffa965d4",
      "ef2f1adf178243bab9d2ef5af8025e5a",
      "61dd735275224c9daa965656900efbfa",
      "65381c65b3e446b88e3ada227c146128",
      "1c9280b066914e1e913be4164fda4b0e",
      "a93c0782f6004e68bac00fbc9688942e",
      "5dad326da38b49cf981109db86ec5580",
      "7e14cdb9e6324b1facf8e9af257ca69f",
      "03f4560a498147ed95fb0aa1da5968a7",
      "dd0482cf3e8746898f71a0469c885c7a",
      "3b90848224084076965ecd1cdadb033a",
      "c5e5c2c34b4d435a91e5912d82e4e9e9",
      "1181ac1cfa2b4887a2895b47cb395ad3",
      "7c4a789394d6490db01c658ee845442c",
      "e9e9ef3df89f4b36bbfbcd39b24fa35c",
      "ae047dc82a5f46038599f772747e49e3",
      "0e43af236173459285d9971c6db13100",
      "6d68072c5f714ca983efd3ddf4fb0df3",
      "1698693ebd174fc48f0d7bb60a4fbd0a"
     ]
    },
    "executionInfo": {
     "elapsed": 10964,
     "status": "ok",
     "timestamp": 1766060228680,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "ebdbdf58",
    "outputId": "8cf5aa97-2553-4526-d3b8-aadffc26ffed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n",
      "{'tokens': ['ØªØºÛŒÛŒØ±Ù…Ø³ÛŒØ±', 'Ù…Ù‡ØªØ±', '(', 'Ø®Ø±Ù…\\u200cØ¢Ø¨Ø§Ø¯', ')'], 'ner_tags': [0, 5, 6, 6, 6], 'langs': ['fa', 'fa', 'fa', 'fa', 'fa'], 'spans': ['LOC: Ù…Ù‡ØªØ± ( Ø®Ø±Ù…\\u200cØ¢Ø¨Ø§Ø¯ )']}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wikiann', 'fa')\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "This cell initializes the tokenizer from the pretrained ParsBERT model. The tokenizer is responsible for converting Persian text into token IDs and subword representations that can be processed by the transformer-based NER model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "82864edc0f514d68b61567c574f2857d",
      "838dfc30fa1746078d91fa46ae9c2171",
      "472f407150d24f8aad690f9a4eba478a",
      "b99d25ef031c4bc09539c16545059a20",
      "edf58907fa3544aaad6a9dab362d09db",
      "1075366f8f694263876d682aa8b2cd15",
      "56aab81c3b1047eb9ee93834f93496e9",
      "7c209a2eb8b949e19ad336a56e155113",
      "a20edb96fe454a36a37098f6feffb847",
      "ec308b55167441ccbe6de5c40f7e24da",
      "e58cd294e5dc416486c9fc78b9856634",
      "30a17cabe5144d07a9f8d340621f0272",
      "798c5970baf241529c47ef221205c239",
      "baa6eac83f8e4213804b8f26ef646724",
      "ab87e83acfe5431b87f5580b604b7532",
      "ab092556a54d49239d90ff4caf5d9dee",
      "b96361fbc41e4b309710f7bc72c8aead",
      "0f3e31c7684448ceadd1623f6be23e4b",
      "4a16c5351cd54eeeb01fb80239855a16",
      "ddc76d4d18e24f85accec57f87815c73",
      "0209d413fa21479a9cbc7e3d84dfca6d",
      "1aa4b6a38ec949ea9e129c9804d8ea21"
     ]
    },
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1766060230311,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "9717910c",
    "outputId": "2f6c692f-c134-41a7-ad3a-8689b7c4c30f"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "This cell defines a preprocessing function that tokenizes the input tokens and aligns the original NER labels with the subword tokens produced by the tokenizer. Special tokens and non-initial subword pieces are assigned a label of `-100` so they are ignored during loss computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11",
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1766060230365,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "3fdb99e3"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "This cell applies the tokenization and label-alignment function to the entire dataset. It processes the data in batches and removes the original columns, producing tokenized datasets that are ready to be used for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "9ccfa6883de94875aea015cf54cb10eb",
      "b5f3a3300f6f4ede9257098447062aa2",
      "c4ef69416e8e4f3481484ba2dba7f22b",
      "41f5021dfdd5482d8ccf1253f3f8014f",
      "75f7d0778bfe4501bc4cfbf9ba90ef59",
      "7a5e187cbc4847b8b657d6818dd8f69e",
      "9b59008a34b5430384b392cdf31f7137",
      "a2f3e08e88d44abc8d86542bcfeff074",
      "158335d4db674b1ca5a8e4a48b779287",
      "37335857ab7c45c2be33e585fcaf7570",
      "e87824c861c14e61b9d0bb5ff63c9327",
      "b9d819ed2c49422ab5cf5b407e234dea",
      "c9ce9aea5509490b9115e52aef4f8b67",
      "0b048f1ffcdc4026a75f4daa49c6be2f",
      "2c983f178323408787c673369596d0f1",
      "f45cbfc7d02b420a82393d7a439ea514",
      "db90632c880a4612a1b2617c3119d77e",
      "4cdc8f05d8604c0abfe6578ae6bebc4c",
      "67c24123d2954e5caa2fb18db72bce9b",
      "e2ec45e8acde42128130c37ff1311dcd",
      "18111f14d239458ca010178abb7f8250",
      "dbe468d8fd15472a83b9d1606b645472",
      "2e292b3c2987461ba3a15805a5c932f0",
      "faf76554fc934836a9b74b587d6a8127",
      "9127c3f61c6140e49ced200b70982a27",
      "d54ef960526a44de80b667a1538fe3a7",
      "785c8ef1ca384aea9d41c6b2412943cb",
      "cb2d321dd7c149cf844df839ad818111",
      "aa9c30846ec74b1e91f572de5e74e4a3",
      "3d9f35b1d1b34abd9a5ee466c548e628",
      "dd640947f8e04380988b58468f20d28f",
      "9fd22455a16747d68ef8381029d03665",
      "8df16f0a5a93450f998e8b4b59c76723"
     ]
    },
    "executionInfo": {
     "elapsed": 3524,
     "status": "ok",
     "timestamp": 1766060233892,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "1af1c38d",
    "outputId": "6396df2f-f5d5-4da5-be16-02f6ad8651d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 13397.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This cell inspects the first sample from the tokenized training dataset to verify that the input tokens, attention masks, and aligned NER labels have been processed correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766060233898,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "e281f1a3",
    "outputId": "a3fac1af-e78e-4bda-c28b-8df84ffbec63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2671, 85815, 61044, 9, 19530, 10, 4],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 0, -100, 5, 6, 6, 6, -100]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This cell extracts the list of NER label names from the training dataset and computes the total number of unique labels. These labels are later used to configure the token classification model and to correctly interpret the modelâ€™s predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1766060233904,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "323760ea",
    "outputId": "9fb9afdb-b920-431e-9150-700542d4d064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
      "Num labels: 7\n"
     ]
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(\"Labels:\", label_list)\n",
    "print(\"Num labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "This cell initializes a transformer-based token classification model using the pretrained ParsBERT checkpoint. It configures the model with the correct number of NER labels and defines mappings between label IDs and label names, enabling proper training and human-readable predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "5c2d72a71bd740249a682354abfd7351",
      "2e70bbdef27f445fa411bee5af544650",
      "8e53837a434344ee96e2649b116d4831",
      "c0cd1cacc4034da090f1e8ec8c8d9030",
      "f53bbf5ef97d4f1f95d777b5f3e7922d",
      "b28851801f354eb795741639e35ac74f",
      "ecbc4d4e9f2e4a6db780e873f6c19bfe",
      "9f62140642714712b38f406def232343",
      "2335b21228d848cc99ef221a716c251e",
      "dd94ce9968ac479b91edcf17cd6b5bbf",
      "a7212532fc484ede83b436c1c2b60446"
     ]
    },
    "executionInfo": {
     "elapsed": 15767,
     "status": "ok",
     "timestamp": 1766060249672,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "f32d1d02",
    "outputId": "5c63729e-81a1-4940-cd11-f573ae80ecff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_checkpoint,\n",
    "    num_labels = num_labels,\n",
    "    id2label = {i:l for i, l in enumerate(label_list)},\n",
    "    label2id = {l:i for i, l in enumerate(label_list)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1766060249725,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "72c37d35",
    "outputId": "154a2cd9-4b75-487a-a216-f30ce652898a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(100000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "This cell creates a data collator specifically designed for token classification tasks. It dynamically pads input sequences and their corresponding labels to the same length within each batch, ensuring efficient and correct batching during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1766060249788,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "b6870ac2"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1766060249789,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "d735623c",
    "outputId": "a646c221-f2e1-43c3-a0bf-2e3ee935b6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available else 'mps' if torch.mps.is_available else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This cell defines the training configuration using Hugging Face `TrainingArguments`. It specifies key hyperparameters such as learning rate, batch size, number of epochs, evaluation and checkpointing strategy, and enables mixed-precision (FP16) training to improve performance and reduce memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54bbe42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner-wikiann-fa\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    fp16 = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1766060249977,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "8bf493fd",
    "outputId": "3158bed2-8b08-4987-fb46-b49ba42e87ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "This cell defines the evaluation metric for the NER task using `seqeval`. It converts model outputs into label predictions, filters out ignored tokens (`-100`), and computes standard NER metrics including precision, recall, F1-score, and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "5886a0e8113a495d8d63e6b82c0c9113",
      "f5f35ed41cb94bb6a0ff114ddf0c3b67",
      "7605f025a82042428690e63a4918d261",
      "329698e77eb8490ba860f2cb247bf15b",
      "217b57c7943b4c6090a0bae7cf8f10c9",
      "a783f5f204864941a14fbe402c0a0364",
      "ab6ef719fc4f4154969245e905a57933",
      "18636e6dff2b435abe05986a0b7b6b73",
      "375c3d9fc4694a3ebf4a3bda91ddb694",
      "addd834d2fd84a6b8d783b9b321a53fc",
      "dbeed9fc1dd74c3994653f2a106b2bc5",
      "2bd355b974c2431d867f3f17cacb76ce",
      "41f01803830f4a11b4494b56eb66e2fd",
      "1f5cb7aa8feb4a439ca2370d0507bc14",
      "a726d443820f48a4a7db7625a51be9bf",
      "f6c0a45e47b7491f8e621c1503f70cec",
      "957c5fc685e04bd6b671f64db4b0c451",
      "78a9aa9c434047a394f87973c5e1b91f",
      "57e196e3729749ccbf7b4a4132cc1b72",
      "0ef2bb12d60f4f1e943008dfec823144",
      "985967a5f02a49689ad38979d2b555a9",
      "e3c0b765f70044c08a73bef87455c9f6"
     ]
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1766060250788,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "4a8387be",
    "outputId": "33e0ead6-0489-4845-a45f-a4cac7eab3dd"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(pred, lab) if l != -100] for (pred , lab) in zip(predictions, labels)]\n",
    "\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(pred, lab) if l != -100] for (pred, lab) in zip(predictions, labels)]\n",
    "\n",
    "\n",
    "    results = metric.compute(\n",
    "        predictions = true_predictions,\n",
    "        references = true_labels\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This cell initializes the Hugging Face `Trainer` with the model, training arguments, datasets, tokenizer, and evaluation function. It then starts the fine-tuning process on the Persian NER dataset, performing training and evaluation at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 721256,
     "status": "ok",
     "timestamp": 1766060972047,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "19fa0dee",
    "outputId": "972544b3-005f-4aa9-8609-0d0699267be9"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "This cell evaluates the fine-tuned NER model on the validation dataset and reports performance metrics such as precision, recall, F1-score, and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 10702,
     "status": "ok",
     "timestamp": 1766060982743,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "84c7e148",
    "outputId": "de27bbc4-f976-43e2-974d-baa06b11c6b2"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "This cell evaluates the trained NER model on different dataset splits. It reports performance metrics for the training set, validation set, and test set, allowing a comprehensive comparison of the modelâ€™s generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 34965,
     "status": "ok",
     "timestamp": 1766061532015,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "duSjtxJHd1bB",
    "outputId": "429af283-73d5-4f30-c949-4baf18f4e074"
   },
   "outputs": [],
   "source": [
    "train_evaluate = trainer.evaluate()\n",
    "validation_evaluate = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "test_evaluate = trainer.evaluate(eval_dataset=tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "This cell aggregates the evaluation results from the training, validation, and test sets into a single dictionary and saves them as a CSV file. This allows the modelâ€™s performance metrics to be easily reviewed, compared, and reused for reporting or documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1766061569912,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "sa8xRa_Tc4cx"
   },
   "outputs": [],
   "source": [
    "all_evaluate = {\n",
    "    'train': train_evaluate,\n",
    "    'validation': validation_evaluate,\n",
    "    'test': test_evaluate\n",
    "}\n",
    "pd.DataFrame(all_evaluate).to_csv('../data/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1766061597412,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "aKi5Z6J2eQHi",
    "outputId": "0a088751-6a78-4d4d-ba2c-b7b7eb64efd4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eval_loss</td>\n",
       "      <td>0.164885</td>\n",
       "      <td>0.164885</td>\n",
       "      <td>0.179350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eval_precision</td>\n",
       "      <td>0.935195</td>\n",
       "      <td>0.935195</td>\n",
       "      <td>0.938316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eval_recall</td>\n",
       "      <td>0.942941</td>\n",
       "      <td>0.942941</td>\n",
       "      <td>0.943120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eval_f1</td>\n",
       "      <td>0.939052</td>\n",
       "      <td>0.939052</td>\n",
       "      <td>0.940712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.972945</td>\n",
       "      <td>0.972945</td>\n",
       "      <td>0.972046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eval_runtime</td>\n",
       "      <td>12.019700</td>\n",
       "      <td>11.628200</td>\n",
       "      <td>11.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eval_samples_per_second</td>\n",
       "      <td>831.967000</td>\n",
       "      <td>859.981000</td>\n",
       "      <td>881.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eval_steps_per_second</td>\n",
       "      <td>51.998000</td>\n",
       "      <td>53.749000</td>\n",
       "      <td>55.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>epoch</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unnamed: 0       train  validation        test\n",
       "0                eval_loss    0.164885    0.164885    0.179350\n",
       "1           eval_precision    0.935195    0.935195    0.938316\n",
       "2              eval_recall    0.942941    0.942941    0.943120\n",
       "3                  eval_f1    0.939052    0.939052    0.940712\n",
       "4            eval_accuracy    0.972945    0.972945    0.972046\n",
       "5             eval_runtime   12.019700   11.628200   11.341100\n",
       "6  eval_samples_per_second  831.967000  859.981000  881.751000\n",
       "7    eval_steps_per_second   51.998000   53.749000   55.109000\n",
       "8                    epoch    5.000000    5.000000    5.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "This cell saves the fine-tuned NER model, tokenizer, and label mappings to disk for later inference or deployment. It also packages the saved files into a ZIP archive and uploads them to Google Drive, ensuring the trained model is safely stored and easily transferable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50947,
     "status": "ok",
     "timestamp": 1766062423405,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "A6p9zS-kguyn",
    "outputId": "150bbc4c-7116-4341-d9ec-9d7d8754f9b4"
   },
   "outputs": [],
   "source": [
    "model_save_path = \"ner_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "with open(f\"{model_save_path}/labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_list, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "notebook_name = \"model_ner_wikiann-fa.ipynb\"\n",
    "!cp \"{notebook_name}\" ner_model/\n",
    "\n",
    "\n",
    "!zip -r ner_model.zip ner_model\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "!cp ner_model.zip /content/drive/MyDrive/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1766062483716,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "TrOBy0K0huMj",
    "outputId": "0daccdc2-26c9-4956-85f4-d16a74431650"
   },
   "outputs": [],
   "source": [
    "# Save the current notebook to Google Drive (manually)\n",
    "from google.colab import files\n",
    "files.download('ner_model.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1766062600977,
     "user": {
      "displayName": "mehdi ghelich",
      "userId": "03998174909181546459"
     },
     "user_tz": -210
    },
    "id": "Pp_Rq7dkiKJX",
    "outputId": "2ffe0c2d-f9cb-4231-9292-b5f9d6d3a511"
   },
   "outputs": [],
   "source": [
    "files.download('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "This cell loads the saved NER model, tokenizer, and label mappings from disk and defines an inference function for Persian text. The function tokenizes the input sentence, runs the model in inference mode, aligns predictions with original words, and prints the recognized named entities for each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù¾Ø²Ø´Ú©ÛŒØ§Ù†    â†’ B-PER\n",
      "Ø¯ÛŒØ±ÙˆØ²      â†’ O\n",
      "Ø¨Ù‡         â†’ O\n",
      "Ø³Ø§Ø²Ù…Ø§Ù†     â†’ B-ORG\n",
      "Ù…Ù„Ù„        â†’ I-ORG\n",
      "Ù…ØªØ­Ø¯       â†’ I-ORG\n",
      "Ø¯Ø±         â†’ O\n",
      "Ù†ÛŒÙˆÛŒÙˆØ±Ú©    â†’ B-LOC\n",
      "Ø±ÙØª        â†’ O\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = '../ner_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "\n",
    "with open(f\"{model_dir}/labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_list = json.load(f)\n",
    "\n",
    "\n",
    "def predict_ner(sentence):\n",
    "    tokens = sentence.split()\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)[0].numpy()\n",
    "\n",
    "\n",
    "    final_tokens, final_tags = [], []\n",
    "    prev_word_id = None\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is None or word_id == prev_word_id:\n",
    "            continue\n",
    "        final_tokens.append(tokens[word_id])\n",
    "        final_tags.append(label_list[predictions[i]])\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    for token, tag in zip(final_tokens, final_tags):\n",
    "        print(f\"{token:10} â†’ {tag}\")\n",
    "\n",
    "predict_ner(\"Ù¾Ø²Ø´Ú©ÛŒØ§Ù† Ø¯ÛŒØ±ÙˆØ² Ø¨Ù‡ Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ù„Ù„ Ù…ØªØ­Ø¯ Ø¯Ø± Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø±ÙØª\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "p113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
